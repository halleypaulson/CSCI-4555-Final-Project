---
title: "Theory Project"
author: "Halley Paulson and Machi Iwata"
date: "4/18/2022"
output: html_document
---
#How to run code
Make sure keras and tensorflow are installed. Make sure wine data csv is in correct location. This document can then simply be knitted or 
code blocks can be run.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
```

#Data
This data has features pertaining to wine measurements to determine quality.
Entries: 4898
Features: 12

```{r}
#Links:
  #https://archive.ics.uci.edu/ml/datasets/wine+quality
  #https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/

#red_wine = read.csv('wine_data/winequality-red.csv', sep=';')
white_wine = read.csv('wine_data/winequality-white.csv', sep=';')

```

#Regression Model
##Preprocessing and creating sets
```{r}
wine_data = white_wine
labels = data.frame(as.numeric(wine_data$quality))
wine_data = scale(subset(wine_data,select=-c(quality)))
summary(wine_data)
 
sample_size = 3918 #80% of full set for training, 20% for testing
set.seed(4898)

tmp = sample(seq_len(nrow(wine_data)),size = sample_size)
training_data = as.matrix(wine_data[tmp,])
training_labels = as.matrix(labels[tmp,])
testing_data = as.matrix(wine_data[-tmp,])
testing_labels = as.matrix(labels[-tmp,])
```

#Building and evaluating the model
```{r}
#Adding more layers didn't change much, adding more nodes helped decrease loss by around .2, dropout made it perform worse
model = keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = ncol(training_data)) %>%
  layer_dense(units = 100, activation = "relu", kernel_regularizer=keras$regularizers$L1L2(0.01,0.01)) %>%
  layer_dense(units = 100, activation = "relu",kernel_regularizer=keras$regularizers$L1L2(0.01,0.01)) %>%
  layer_dense(units = 100, activation = "relu") %>%
  layer_dense(units = ncol(training_labels), activation = "linear")

model %>% compile(
  loss = 'mean_squared_error', #finds average squared difference between predicted and actual
  optimizer = 'adam',
)

#summary(model)

model %>% 
fit(
  x = training_data,
  y = training_labels,
  epochs = 50, #trials showed that more epochs = lower loss
   #smaller batches in combination with high epochs was beneficial
)

model %>% evaluate(testing_data,testing_labels)

predictions = model %>% predict(testing_data)
comparison = data.frame(cbind(actual=testing_labels,predicted=predictions))
colnames(comparison) = c('Actual','Predicted')

head(comparison)
```

#Classification Model
##Preprocessing and creating sets
```{r}
wine_data = white_wine #tried with full set of features and half and it didn't effect loss much. Decided to keep full set
labels = data.frame(to_categorical(wine_data$quality))
wine_data = scale(subset(wine_data,select=-c(quality)))
 
sample_size = 3000 #80% of full set for training, 20% for testing
set.seed(4898)

tmp = sample(seq_len(nrow(wine_data)),size = sample_size)
training_data = as.matrix(wine_data[tmp,])
training_labels = as.matrix(labels[tmp,])
testing_data = as.matrix(wine_data[-tmp,])
testing_labels = as.matrix(labels[-tmp,])
```

#Building and evaluating the model
```{r}
# Adding dropout made this perform worse by a lot
model = keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = ncol(training_data)) %>%
  layer_dense(units = 100, activation = "relu",kernel_regularizer=keras$regularizers$L1(0.01)) %>%
  layer_dense(units = 100, activation = "relu") %>%
  layer_dense(units = ncol(training_labels), activation = "softmax")

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
)

model %>% 
fit(
  x = training_data,
  y = training_labels,
  epochs = 50, #similar to regression model, more means lower loss and better accuracy
)

model %>% evaluate(testing_data, testing_labels)

predictions = model %>% predict(testing_data)

head(predictions)
head(testing_labels)

```








