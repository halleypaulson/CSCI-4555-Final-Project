---
title: "Theory Project"
author: "Halley Paulson and Machi Iwata"
date: "4/18/2022"
output: html_document
---
#How to run code
Make sure keras and tensorflow are installed. Make sure wine data csv is in correct location. This document can then simply be knitted or 
code blocks can be run.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
```

#Data
This data has features pertaining to wine measurements to determine quality.
Entries: 4898
Features: 12

```{r}
#Links:
  #https://archive.ics.uci.edu/ml/datasets/wine+quality
  #https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/

#red_wine = read.csv('wine_data/winequality-red.csv', sep=';')
white_wine = read.csv('wine_data/winequality-white.csv', sep=';')

```

#Regression Model
##Preprocessing and creating sets
```{r}
wine_data = white_wine #tried with full set of features and half and it didn't effect loss much. Decided to keep full set
wine_data$quality = as.numeric(wine_data$quality)

sample_size = 3918 #80% of full set for training, 20% for testing
set.seed(4898)

tmp = sample(seq_len(nrow(wine_data)),size = sample_size)
training = wine_data[tmp,]
testing = wine_data[-tmp,]

training_data = as.matrix(subset(training, select = -c(quality)))
training_labels = as.matrix(subset(training, select = c(quality)))

testing_data = as.matrix(subset(testing, select = -c(quality)))
testing_labels = as.matrix(subset(testing, select = c(quality)))

#normalizing features since features had different ranges. Trying without results in high loss, but the training loss and evaluation loss are now the same?
training_data = scale(training_data)
testing_data = scale(testing_data)
```

#Building and evaluating the model
```{r}
#Adding more layers didn't change much, adding more nodes helped decrease loss by around .2, dropout made it perform worse
model = keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = ncol(training_data)) %>%
  layer_dense(units = 100, activation = "relu") %>%
  layer_dense(units = 100, activation = "relu") %>%
  layer_dense(units = 100, activation = "relu") %>%
  layer_dense(units = ncol(training_labels), activation = "linear")

model %>% compile(
  loss = 'mean_squared_error', #finds average squared difference between predicted and actual
  optimizer = 'adam',
)

#summary(model)

model %>% 
fit(
  x = training_data,
  y = training_labels,
  epochs = 60, #trials showed that more epochs = lower loss
  batch_size = 15 #smaller batches in combination with high epochs was beneficial
)

model %>% evaluate(testing_data,testing_labels) #training loss is around .1 but evaluated loss is around .5, why?

predictions = model %>% predict(testing_data)
comparison = data.frame(cbind(actual=testing_labels,predicted=predictions))
colnames(comparison) = c('Actual','Predicted')

head(comparison)
```

#Classification Model
##Preprocessing and creating sets
```{r}
wine_data = white_wine
wine_data$quality = to_categorical(wine_data$quality)

sample_size = 3918 #80% of full set for training, 20% for testing
set.seed(4898)

tmp = sample(seq_len(nrow(wine_data)),size = sample_size)
training = wine_data[tmp,]
testing = wine_data[-tmp,]

training_data = as.matrix(subset(training, select = -c(quality)))
training_labels = as.matrix(subset(training, select = c(quality)))

testing_data = as.matrix(subset(testing, select = -c(quality)))
testing_labels = as.matrix(subset(testing, select = c(quality)))

#normalizing features since features had different ranges
training_data = scale(training_data)
testing_data = scale(testing_data)
```

#Building and evaluating the model
```{r}
# Adding dropout made this perform worse by a lot
model = keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = ncol(training_data)) %>%
  layer_dense(units = 100, activation = "relu") %>%
  layer_dense(units = 100, activation = "relu") %>%
  layer_dense(units = 100, activation = "relu") %>%
  layer_dense(units = ncol(training_labels), activation = "softmax")

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = 'accuracy'
)

model %>% 
fit(
  x = training_data,
  y = training_labels,
  epochs = 60, #similar to regression model, more means lower loss and better accuracy
  batch_size = 50 #keeping this smaller (50 vs 100) improved performance
)

model %>% evaluate(testing_data, testing_labels) #results from training still different from evaluated, why?

predictions = model %>% predict(testing_data)

head(predictions)
head(testing_labels)

```









